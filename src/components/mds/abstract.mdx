# Abstract

Large language models (LLMs) have revolutionized robots through their language understanding and task planning capabilities, enabling the conversion of human instructions into executable robot policies (move(), grasp(), etc.) for manipulation. However, this integration introduces a security concern: LLMs’ vulnerability to jailbreak attacks is amplified when threats transition from generating malicious text in the digital domain to executing harmful actions in the physical world.

Yet, it is still unknown whether and how traditional jailbreaks apply to LLM-based robots. To better understand the gap between traditional LLM jailbreaks and LLM-based robot jailbreaks, we first create Harmful-RLbench, a “human-object-environment” robot safety risk-oriented dataset, and then conduct a measurement study on 20 mainstream LLM-based robots. We reveal that traditional LLM jailbreak attacks are largely inapplicable in robot scenarios due to ignoring the robotic characteristics during the optimization and assessment of jailbreak vectors, resulting in generated policies that cannot be executed by robots. To bridge this gap, we present POEX (POlicy EXecutable), an automated red-teaming framework. First, to make policies executable, we leverage unaligned LLMs’ hidden-layer gradients as optimization directions to guide victim LLMs toward generating harmful and executable policies. Second, to accurately assess the executability of policies, we introduce the multi-agent evaluator. Experiments conducted on real-world robotic systems and in simulation demonstrate POEX’s practical effectiveness and transferability across LLMs, highlighting physical security vulnerabilities in LLM-based robots. We also implement two defense strategies to mitigate such threats. Our work underscores the urgent need for security measures to ensure the safe deployment of LLM-based robots in critical applications.